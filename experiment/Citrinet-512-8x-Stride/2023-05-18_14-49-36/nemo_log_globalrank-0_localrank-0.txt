[NeMo W 2023-05-18 14:49:23 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2023-05-18 14:49:23 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2023-05-18 14:49:32 mixins:170] Tokenizer SentencePieceTokenizer initialized with 512 tokens
[NeMo I 2023-05-18 14:49:32 ctc_bpe_models:63] 
    Replacing placeholder number of classes (-1) with actual number of classes - 512
[NeMo I 2023-05-18 14:49:33 collections:193] Dataset loaded with 28165 files totalling 31.76 hours
[NeMo I 2023-05-18 14:49:33 collections:194] 1 files were filtered totalling 0.01 hours
[NeMo I 2023-05-18 14:49:33 collections:193] Dataset loaded with 10408 files totalling 12.66 hours
[NeMo I 2023-05-18 14:49:33 collections:194] 0 files were filtered totalling 0.00 hours
[NeMo I 2023-05-18 14:49:34 collections:193] Dataset loaded with 10444 files totalling 12.50 hours
[NeMo I 2023-05-18 14:49:34 collections:194] 0 files were filtered totalling 0.00 hours
[NeMo I 2023-05-18 14:49:34 features:287] PADDING: 16
[NeMo I 2023-05-18 14:49:36 exp_manager:374] Experiments will be logged at /home/nouran.ali/asr/commonvoice/code/experiment/Citrinet-512-8x-Stride/2023-05-18_14-49-36
[NeMo I 2023-05-18 14:49:36 exp_manager:797] TensorboardLogger has been set up
[NeMo I 2023-05-18 14:49:39 modelPT:722] Optimizer config = Novograd (
    Parameter Group 0
        amsgrad: False
        betas: [0.8, 0.25]
        eps: 1e-08
        grad_averaging: False
        lr: 0.05
        weight_decay: 0.001
    )
[NeMo I 2023-05-18 14:49:39 lr_scheduler:910] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fdda5269430>" 
    will be used during training (effective maximum steps = 1760500) - 
    Parameters : 
    (warmup_steps: 1000
    warmup_ratio: null
    min_lr: 1.0e-05
    last_epoch: -1
    max_steps: 1760500
    )
[NeMo I 2023-05-18 14:50:10 modelPT:722] Optimizer config = Novograd (
    Parameter Group 0
        amsgrad: False
        betas: [0.8, 0.25]
        eps: 1e-08
        grad_averaging: False
        lr: 0.05
        weight_decay: 0.001
    )
[NeMo I 2023-05-18 14:50:10 lr_scheduler:910] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fdd9e02aa60>" 
    will be used during training (effective maximum steps = 1760500) - 
    Parameters : 
    (warmup_steps: 1000
    warmup_ratio: null
    min_lr: 1.0e-05
    last_epoch: -1
    max_steps: 1760500
    )
